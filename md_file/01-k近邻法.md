# *k* 近邻法(*k*-nearest neighbor, *k*-NN)

## *1、k 近邻算法*

*k*近邻法是一种基本的分类与回归算法。

**算法**：

输入：训练数据集

$$
T={(x_1,y_1),(x_2,y_2),\cdots(x_N,y_N)}
$$

其中，$x_i\in\chi(样本空间或状态空间)\subseteq R^n$为实例的特征向量;

$y_i\in\Upsilon(假设空间)=\{c_1,c_2,\cdots,c_K\}$为实例的类别，$i=1,2,\cdots,N$;

实例特征向量$x$

输出：实例$x$所属的类*y*

（1）根据给定的距离度量，在训练集*T*中找出与$x$最邻近的*k*个点，涵盖这*k*个点的$x$的邻域记作$N_k(x)$;
（2）在$N_k(x)$中根据分类决策规则（如多数表决）决定$x$的类别*y*：

$$
y=\argmax \limits_{c_j} \sum \limits_{x_i\in N_k(x)} {I(y_i=c_j), i=1,2,\cdots,N;j=1,2,\cdots,K}
$$

式中，*I*为指示函数，即当$y_i=c_j$时，*I*为1，否则为0。
*k*近邻法没有显示的学习过程。

## 2、*k* 近邻模型

*k*近邻法使用的模型实际上对应于**对特征空间的划分**。
模型通常由**三个基本要素**——距离度量、*k*值的选择和分类决策规则决定。

### 2.1、模型

根据三要素将训练集特征空间划分子空间，确定子空间里的每个点所属的类。

### 2.2、距离度量

特征空间中两个实例点的距离是两个实例点相似程度的反映。
设特征空间$\chi$是$n$维实数向量空间$R^n$，$x_i,x_j\in\chi$，$x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T$，$x_j=(x_j^{(1)},x_j^{(2)},\cdots,x_j^{(n)})^T$
$x_i,x_j$的$L_p$距离定义为

$$
L_p(x_i,x_j)=\left (\sum\limits_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p\right)^{\frac{1}{p}}
$$

这里$p\geq1$。当$p=2$时，称为欧式距离(Euclidean distance)，即

$$
L_2(x_i,x_j)=\left (\sum\limits_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^2\right)^{\frac{1}{2}}
$$

当$p=1$时，称为曼哈顿距离(Manhattan distance)，即

$$
L_1(x_i,x_j)=\sum\limits_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|
$$

当$p=\infty$时，它是各个坐标轴的最大值，即

$$
L_\infty(x_i,x_j)=\max_l|x_i^{(l)}-x_j^{(l)}|
$$

**例题**：已知二维空间的3个点$x_1=(1,1)^T$，$x_2=(5,1)^T$，$x_3=(4,4)^T$，试求在$p$取不同值时，$L_p$距离下$x_1$的最近邻点。
**解**：$x_2$与$x_1$的距离：因为只有第一个维度不同，所以$L_p=4$
$x_3$与$x_1$的距离：$L_1=|4-1|+|4-1|=6$，
$L_2=\sqrt{|4-1|^2+|4-1|^2}=3\sqrt{2}=4.24$，
$L_3=\sqrt[3]{|4-1|^3+|4-1|^3}=3\sqrt[3]{2}=3.57$
于是：当$p\in(1,2)$，$x_2$是$x_1$的最近邻点；当$p\geq3$时，$x_3$是$x_1$的最近邻点。

### 2.3、*k*值的选择

小$k$值，相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差会减小，但“学习”的估计误差会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰好是噪声，预测就会出错。
**$k$值的减小意味着整体模型变复杂，容易发生过拟合。**
大$k$值，相当于用较大的邻域中的训练实例进行预测，“学习”的估计误差会减小（全部预测为正），但“学习”的近似误差会增大，与输入实例较远的（不相似的）训练实例也会对预测起作用。
**$k$值的增大意味着整体模型变简单，容易发生欠拟合。**
在应用中，$k$值一般取一个较小的数值。通常采用交叉验证法来选取最优的$k$值。

### 2.4、分类决策规则

| 问题 | 等权     | 权重与距离成反比          |
| ---- | -------- | ------------------------- |
| 分类 | 多数表决 | 加权多数表决              |
| 回归 | 平均值法 | 加权平均值法（权重和为1） |

## 3、*k* 近邻法的实现

如何对训练数据进行快速的$k$近邻搜索？当特征空间维数大，训练数据容量大时？

| 方法            | 介绍                                     |
| --------------- | ---------------------------------------- |
| 线性扫描(brute) | 最简单的实现方法，数据量大时，非常耗时   |
| kd数(kd tree)   | 特殊结构存储训练数据，减少计算距离的次数 |

### 3.1、构造*kd*树

**算法（构造平衡*kd*树）**:
输入：*k*维空间数据集$T=\{x_1,x_2,\cdots,x_N\}$,其中样本$x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(k)})^T$，$i=1,2,\cdots,N$;
输出：kd树。
（1）开始：构造根节点选择$x^{(1)}$为坐标轴，以T中所有实例的$x^{(1)}$坐标的中位数(两个中位数取大的)为切分点，将根节点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。
左节点对应坐标$x^{(1)}$小于切分点的子区域，右节点对应坐标$x^{(1)}$大于切分点的子区域。
（2）重复：对深度为$j$的节点，选择$x^{(l)}$为切分的坐标轴，$l=j(mod\;k)+1$，将该节点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(l)}$垂直的超平面实现。
（3）直到两个子区域没有实例存在时停止。
**例题**:给定一个二维空间的数据集：

$$
T=\{(2,3)^T,(5,4)^T,(9,6)^T,(4,7)^T,(8,1)^T,(7,2)^T\}
$$

构造一个平衡kd树。
**解**：二维空间数据集，$k=2$;
深度$j=0$,$l=0\;mod\;2 + 1=1$,所以取$x^{(1)}$坐标,中位数为7，(7,2),左子节点(2,3)(5,4)(4,7),右子节点(9,6)(8,1);
深度$j=1$,$l=1\;mod\;2 + 1=2$,所以取$x^{(2)}$坐标,左节点中位数为4，(5,4),右节点中位数为6，(9,6);
深度$j=2$,$l=2\;mod\;2 + 1=1$,所以取$x^{(1)}$坐标,(5,4)子节点中位数为4，(4,7),还剩(2,3)，(9,6)子节点仅剩(8,1);

第0层：--------------(7,2)--------------
第1层：-----(5,4)-------------(9,6)----
第2层：(2,3)(4,7)-------(8,1)

### 3.2、搜索*kd*树

$kd$树搜索的平均计算复杂度是$O(\log N)$，N是训练实例数。
$kd$树更适用于训练实例数远大于空间维数时的*k*近邻搜索。
以**最近邻为例**加以叙述：
给定一个目标点，搜索其最近邻。首先找到包含目标点的叶节点；然后从该叶节点出发，依次回退到父节点；不断查找与目标点最邻近的节点，当确定不可能存在更近的节点时终止。搜索限制在了空间的局部区域，效率大大提高。
**算法**:
输入：已构造的$kd$树，目标点$x$;
输出：$x$的最近邻。

1. 在$kd$树中找到包含目标点的叶节点：从根节点出发，递归向下访问$kd$树。若目标点$x$当前维度的坐标小于切分点的坐标，则左移到左子节点，否则移动到右子节点。直到子节点为叶节点为止。
2. 以此叶节点为“当前最近点”。
3. 递归向上回退，在每个节点进行以下操作：
   1. 如果该结点保存的实例点更近，则以该实例点为“当前最近点”；
   2. 检查该节点的父节点的另一个子节点对应区域是否有更近点，以目标点为球心，当前最近点与目标点的距离为半径的超球体。
      1. 另一个子节点区域与超球体相交，则存在更近的点，以此为当前更近的点，接着递归进行最近邻搜索。
      2. 如果不相交，向上回退。
4. 当回退到根节点时，搜索结束。最后的“当前最近点”即为$x$的最近邻点。

**例题**:给定一个二维空间的数据集：

$$
T=\{(2,3)^T,(5,4)^T,(9,6)^T,(4,7)^T,(8,1)^T,(7,2)^T\}
$$

搜索(3,7)。
**解**：kd树如下
第0层：--------------(7,2)----------------$x^{(1)}$
第1层：-----(5,4)-------------(9,6)------$x^{(2)}$第2层：(2,3)(4,7)-------(8,1)-----------$x^{(1)}$

1. 3<7,左子节点
2. 7>4,找到右子节点(4,7)为当前最近点$L_2=1$
3. 以(2,3)为圆心，$L_2=1$为半径画圆，刚好与(4,7)相切，与(4,7)的父节点(5,4)的另一个子节点(2,3)相离，说明不可能存在更近的节点。
4. 所以(3,7)的最近邻点为(4,7)

## 4、*knn*应用

### 4.1、基于python实现的电影数据的一个KNN

#### 4.1.1、 KNN等权分类

```python
import numpy as np
import pandas as pd
# 训练集数据 [亲吻镜头次数，打斗镜头次数，电影类型（1：爱情，-1：动作）]
T = [[3, 104, -1], [2, 100, -1], [1, 81, -1],
     [101, 10, 1], [99, 5, 1], [98, 2, 1]]
# 预测数据，判断其电影类型
x_test = [16, 94]
# 邻近点
k = 5
distance = [(np.sum((np.array(i[:-1])-np.array(x_test)) ** 2) ** 0.5, i[-1]) for i in T]
distance.sort()
print(distance)
dis_df = pd.DataFrame(distance[:k], columns=['distance', 'labels'])
print(dis_df)
# counts.idxmax()：取最多的index,=1
counts = dis_df['labels'].value_counts()
pred = '动作' if counts.idxmax() else '爱情'
# pred = '动作' if dis_df['labels'].sum() else '爱情'
print(f'(16,94)预测为{pred}片')
```

#### 4.1.2、 KNN加权分类

```python
import numpy as np
import pandas as pd
# 训练集数据 [亲吻镜头次数，打斗镜头次数，电影类型（1：爱情，-1：动作）]
T = [[3, 104, -1], [2, 100, -1], [1, 81, -1],
     [101, 10, 1], [99, 5, 1], [98, 2, 1]]
# 预测数据，判断其电影类型
x_test = [16, 94]
# 邻近点
k = 5
distance = [(np.sum((np.array(i[:-1])-np.array(x_test)) ** 2) ** 0.5, i[-1]) for i in T]
distance.sort()
print(distance)
dis_df = pd.DataFrame(distance[:k], columns=['distance', 'labels'])
weights_sum = np.sum(pd.Series(1 / dis_df['distance']))

dis_df['weight'] = (1 / dis_df['distance']) / weights_sum
print(dis_df)
if np.sum(dis_df[dis_df['labels'] == 1]['weight']) > np.sum(dis_df[dis_df['labels'] == -1]['weight']):
    pred = '爱情'
else:
    pred = '动作'
print(f'(16,94)预测为{pred}片')
```

#### 4.1.3、 KNN等权分类封装

```python
"""简单实现一下等权分类，封装成KNN类，实现fit，predict，score方法"""
import numpy as np
import pandas as pd


class KNN:
    def __init__(self, k):
        """
        :param k: 近邻点数
        """
        self.k_neighbors = k

    def fit(self, train_x, train_y):
        """
        KNN算法训练模型即为存储训练集数据，已在构造函数中储存
        此处还可定义其它存储方法，比如kd树
        """
        self.train_x = np.array(train_x)
        self.train_y = np.array(train_y)

    def get_k_neighbors(self, test_x):
        """
        获取前k个临近点
        :return: 前k个临近点的索引位置和标签
        """

        # 计算距离
        def cal_dis(X, y):
            return np.sum((X - y) ** 2, axis=1) ** 0.5

        labels = []
        for test in test_x:
            dis = cal_dis(self.train_x, test)
            dis_df = pd.Series(dis, index=[i for i in range(len(dis))])
            # 排序
            dis_df.sort_values(inplace=True)
            # 前k个邻近数据点的索引
            dis_k_id = dis_df[:self.k_neighbors].index
            # 前k个标签
            k_labels = [self.train_y[i] for i in dis_k_id]
            labels.append(k_labels)
        return labels

    def predict(self, x, is_weight=False):
        """
        多数表决
        is_weight 是否采用加权投
        return X对应的预测标签列表
        """
        labels = pd.DataFrame(self.get_k_neighbors(x))
        # labels['Row_sum'] = labels.apply(lambda x: x.sum(), axis=1)  # 按行求和，添加为新列
        # labels['Pred'] = labels.apply(lambda x: 1 if x['Row_sum'] > 0 else -1, axis=1)
        labels['Pred'] = labels.apply(lambda x: x.value_counts().idxmax(), axis=1)
        # print(labels)
        return labels['Pred'].values

    def score(self, x, y):
        """准确率"""
        y_hat = self.predict(x)
        acc = np.mean(y_hat == y)
        return acc


if __name__ == '__main__':
    # 训练数据
    T = [[3, 104, -1], [98, 2, 1], [2, 100, -1],
         [101, 10, 1], [99, 5, 1], [1, 81, -1]]
    T = np.array(T)
    train_x, train_y = (T[:, :-1], T[:, -1])
    # 预测数据，判断其电影类型
    test_x = [[18, 90], [50, 50]]
    knn = KNN(3)
    # 训练
    knn.fit(train_x, train_y)
    # 预测
    print('预测结果：{}'.format(knn.predict(test_x)))
    # # 准确率
    print('预测结果：{}'.format(knn.predict(train_x)))
    print('预测准确率为：{}'.format(knn.score(train_x, train_y)))
    print('-----------下面测试一下鸢尾花数据-----------')
    from sklearn.datasets import load_iris
    from sklearn.model_selection import train_test_split

    X, Y = load_iris(return_X_y=True)
    print(X.shape, Y.shape)
    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)
    knn2 = KNN(k=3)
    knn2.fit(x_train, y_train)
    print('测试集预测结果：{}'.format(knn2.predict(x_test)))
    print('测试集预测准确率为：{}'.format(knn2.score(x_test, y_test)))
    print('训练集预测结果：{}'.format(knn2.predict(x_train)))
    print('预训练集测准确率为：{}'.format(knn2.score(x_train, y_train)))
```

#### 4.1.4、 KNN加权分类封装(kd树)

```python
"""实现加权分类，封装成KNN类(kd树)，实现fit，predict，score方法"""
import numpy as np
import pandas as pd
from sklearn.neighbors import KDTree
from sklearn.metrics import accuracy_score


class KNN:
    def __init__(self, k, is_weight, with_kd_tree=True):
        """
        :param k: 近邻点数
        """
        self.kd_tree = None
        self.train_y = None
        self.train_x = None
        self.k_neighbors = k
        self.is_weight = is_weight
        self.with_kd_tree = with_kd_tree

    def fit(self, train_x, train_y):
        """
        KNN算法训练模型即为存储训练集数据，已在构造函数中储存,此处还可定义其它存储方法，比如kd树
        """
        self.train_x = np.array(train_x)
        self.train_y = np.array(train_y)
        if self.with_kd_tree:
            self.kd_tree = KDTree(self.train_x, leaf_size=10, metric='minkowski')

    def cal_dis(self, X, y):
        """
        计算距离
        :param X: 训练集数据点
        :param y: 测试集向量（单个测试点）
        :return: 单个测试点与训练集的距离(len(x),)
        """
        return np.sum((X - y) ** 2, axis=1) ** 0.5

    def get_k_neighbors(self, test_x):
        """
        获取前k个临近点
        :return: 前k个临近点的索引位置和标签
        """
        labels = []
        distance = []
        if self.with_kd_tree:
            # 返回对应最近的k个样本的下标，如果return_distance=True同时也返回距离
            distance, index = self.kd_tree.query(test_x, k=self.k_neighbors, return_distance=True)
            # 获取对应最近k个样本的距离
            for i in index:
                labels.append(self.train_y[i])
            return distance, labels
        else:
            for test in test_x:
                dis = self.cal_dis(self.train_x, test)
                dis_df = pd.Series(dis, index=[i for i in range(len(dis))])
                # 排序
                dis_df.sort_values(inplace=True)
                distance.append(dis_df[:self.k_neighbors])
                # 前k个邻近数据点的索引
                dis_k_id = dis_df[:self.k_neighbors].index
                # 前k个标签
                k_labels = [self.train_y[i] for i in dis_k_id]
                labels.append(k_labels)
            return distance, labels

    def predict(self, x):
        """
        多数表决
        is_weight 是否采用加权投
        return X对应的预测标签列表
        """
        labels = pd.DataFrame(self.get_k_neighbors(x)[1])

        if not self.is_weight:
            # 取每行标签中出现次数最多的那个标签
            labels['counts'] = labels.apply(lambda x: x.value_counts().idxmax(), axis=1)
            print(labels)
            return labels['counts'].values
        else:
            # 距离的倒数
            distance_reciprocal = 1 / (np.array(self.get_k_neighbors(x)[0]) + 0.0001)
            # 权重和
            weights_sum = np.sum(distance_reciprocal, axis=1, keepdims=True)
            # 权重矩阵，[weight1, weight2, weight3]
            weights = distance_reciprocal / weights_sum
            pred = []
            for i in range(len(labels)):
                # 遍历待测样本, [label1, label2, label3]
                x = labels.iloc[i]
                # 遍历权重矩阵，每行为待测样本的权重向量[w1, w2, w3]
                w = weights[i]
                df = pd.DataFrame({'labels': x, 'weights': w})
                # 将[label1, label2, label3] 与 [w1, w2, w3] 按label分组相加并取大的值的标签
                df2 = df.groupby('labels').sum()
                # df2.idxmax()是Series对象，索引‘weights’值，即标签
                pred.append(df2.idxmax()['weights'])
            labels['pred'] = pred
            # print(labels)
            return labels['pred'].values

    def score(self, x, y):
        """准确率"""
        y_hat = self.predict(x)
        acc = np.mean(y_hat == y)
        return acc


if __name__ == '__main__':
    # 训练数据
    T = [[3, 104, -1], [98, 2, 1], [2, 100, -1],
         [101, 10, 1], [99, 5, 1], [1, 81, -1]]
    T = np.array(T)
    train_x, train_y = (T[:, :-1], T[:, -1])
    # 预测数据，判断其电影类型
    test_x = [[18, 90], [50, 50]]
    knn = KNN(3, is_weight=True, with_kd_tree=True)
    # 训练
    knn.fit(train_x, train_y)
    # 预测
    print('预测结果：{}'.format(knn.predict(test_x)))
    # # 准确率
    print('预测结果：{}'.format(knn.predict(train_x)))
    print('预测准确率为：{}'.format(knn.score(train_x, train_y)))
    print('-----------下面测试一下鸢尾花数据-----------')
    from sklearn.datasets import load_iris
    from sklearn.model_selection import train_test_split

    X, Y = load_iris(return_X_y=True)
    print(X.shape, Y.shape)
    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)
    knn2 = KNN(k=3, is_weight=True, with_kd_tree=True)
    knn2.fit(x_train, y_train)
    print('测试集预测结果：{}'.format(knn2.predict(x_test)))
    print('测试集预测准确率为：{}'.format(knn2.score(x_test, y_test)))
    print('训练集预测结果：{}'.format(knn2.predict(x_train)))
    print('预训练集测准确率为：{}'.format(knn2.score(x_train, y_train)))

```

#### 4.1.5、 KNN调包实现

```python
import pandas as pd
from sklearn import neighbors
# 训练集数据
T = [[3, 104, -1], [98, 2, 1], [2, 100, -1],
     [101, 10, 1], [99, 5, 1], [1, 81, -1]]
data = pd.DataFrame(T, columns=['亲吻镜头次数', '打斗镜头次数', '电影类型'])
train_x, train_y = (data.iloc[:, :-1], data.iloc[:, -1])
# 预测数据，判断其电影类型
test_x = pd.DataFrame([[18, 90], [50, 50]], 
                      columns=['亲吻镜头次数', '打斗镜头次数'])
# 邻近点数
k = 3
# 实例化k近邻分类对象
knn01 = neighbors.KNeighborsClassifier(n_neighbors=k)
# 训练模型，存储训练集数据
knn01.fit(train_x, train_y)
# 预测
pred = knn01.predict(test_x)
print(f'预测结果为:{pred}')
# 精度accuracy
acc = knn01.score(X=train_x, y=train_y)
print(f'预测精度为：{acc:.4f}')
```

### 4.2、基于KNN的API实现鸢尾花数据分类

#### 4.2.1、KNN鸢尾花数据分类

```python
import pandas as pd
import numpy as np
from sklearn import neighbors
from sklearn.model_selection import train_test_split
import joblib
# 1、数据加载
names = ['x1', 'x2', 'x3', 'x4', 'y']
data = pd.read_csv('../datas/iris.data', sep=',', names=names)
# 概览数据
print(data.head())
print(data.shape)
print(data["y"].value_counts())  # 分为三类，每类50个样本
# 2、数据进行清洗
# NOTE: 不需要做数据处理
flower = {'Iris-setosa': 1, 'Iris-versicolor': 2, 'Iris-virginica': 3}
data['y'] = data['y'].apply(lambda row: flower[row] if row in flower else 0)
print(data)
data.info()
data['y'] = data['y'].astype(np.int32)
print(data['y'].value_counts())

# 3、获取我们的数据的特征属性X和目标属性Y
X, Y = (data.iloc[:, :-1], data.iloc[:, -1])
print(X.head())

# 4、划分训练集与测试集
# train_size: 训练数据的占比，默认0.75
# random_state：随机数种子，默认为None，使用当前的时间戳；非None值，可以保证多次运行的结果是一致的。
train_x, test_x, train_y, test_y = train_test_split(X, Y, train_size=0.8, random_state=1)
print(f'训练数据X的格式:{train_x.shape}')
print(f'测试数据X的格式:{test_x.shape}')

# 5、特征工程：正则化、标准化、文本的处理
# NOTE: 此处不做特征工程，后续有详细介绍

# 6、构建模型
knn_iris = neighbors.KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='kd_tree')
# 7、训练模型
knn_iris.fit(train_x, train_y)
# 8、模型效果的评估 （效果不好，返回第二步进行优化，达到要求）
pred_train = knn_iris.predict(train_x)
print(f'knn02模型训练集预测结果:{pred_train}')
acc_train = knn_iris.score(train_x, train_y)
print(f"knn02模型：训练集上的精度:{acc_train}")
pred_test = knn_iris.predict(test_x)
print(f'knn02模型预测结果:{pred_test}')
acc_test = knn_iris.score(test_x, test_y)
print(f"knn02模型：测试集上的效果(准确率):{acc_test}")

# 9、模型保存,方便后续调用
joblib.dump(knn_iris, "./knn_iris.m")  # 存储在当前文件夹
```

#### 4.2.2、硬盘持久化模型应用

```python
import joblib
import warnings
warnings.filterwarnings("ignore")
# 1、加载模型,注意模型文件存储路径
knn = joblib.load('./knn_iris.m')
# 2、预测数据
x = [[3.3, 2.1, 4.7, 1.9]]
y_hat = knn.predict(x)
y_hat_prob = knn.predict_proba(x)
print(f'预测为：{y_hat}')
```

### 4.3、比较python和skl包预测的时间

```python
import sys
sys.path.append('../01-基于python实现的电影数据分类/04_KNN加权分类封装/')
from KNN import KNN
import time

"""python测试鸢尾花数据所花时间"""
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
X, Y = load_iris(return_X_y=True)
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)
# 开始计时
since = time.time()
knn1 = KNN(k=3, is_weight=True, with_kd_tree=True)
knn1.fit(x_train, y_train)
pred = knn1.predict(x_test)
acc = knn1.score(x_test, y_test)
# 预测结束，停止计时
time_elapsed = time.time() - since
print(f'python预测用时:{time_elapsed:.4f}s')
# python预测用时:0.0626s
"""python测试鸢尾花数据所花时间"""
from sklearn import neighbors
# 开始计时
since = time.time()
knn2 = neighbors.KNeighborsClassifier(n_neighbors=3, leaf_size=10, algorithm='kd_tree', weights='distance')
knn2.fit(x_train, y_train)
pred1 = knn2.predict(x_test)
acc1 = knn2.score(x_test, y_test)
# 预测结束，停止计时
time_elapsed = time.time() - since
print(f'skl预测用时:{time_elapsed}s')
# skl预测用时:0.0s

```

## 5、*总结*

1. knn算法原理
2. knn模型三要素：k值的选择，距离度量，分类决策规则
3. kd树的构造以及搜索
4. sklearn库的使用
